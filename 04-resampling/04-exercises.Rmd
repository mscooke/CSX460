---
title: "Sensitivity and Specificity"
author: "Your Name Here"
date: "October 5, 2015"
output: html_document
---


## Readings

***APM***

- ***Chapter 5 Measuring Performance in Regression Models*** (esp. ***5.2 The Variance Bias Trade-Off***)  (5 pages)
- ***Chapter 11 Measuring Performance in Classification Models*** (~20 pages)
- ***Chapter 7.4 K-Nearest Neighbors (regression)*** (2 pages)
- ***Chapter 13.5 K-Nearest Neighbors (classification)*** (3 pages)


```{r, echo=FALSE, results='hide', warning=FALSE }
packs <-  c('ggplot2', 'magrittr', 'dplyr', 'caret', 'AppliedPredictiveModeling')

for( nm in packs ) { 
  # message(nm)
  if( ! nm  %in% installed.packages()[,1]  ) install.packages(nm)
  library(nm, character.only = TRUE)
}

.. = NULL  # For Aesthetics

```


## EXERCISE 1: Resampling

`x` is a random variable. We want to not only know what the `mean(x)` is but want to calculate the uncertainty of `mean(x)`.  Measuring the uncertainty requires repeated measurements of `mean(x)`.

- Calculate the mean of `x`.
- Calculte the `sd( mean(x) )` using the **using 10-fold cross-validation**.  Create your own folds, show your work. (An example is for the Bootstrap is given as a hint. )


```{r}
set.seed(1) 
x <- runif(20,1,20)

x_mean = mean(x)

k=10

# BOOTSTRAP (EXAMPLE)
sd_boot <- sapply(1:k, function(i) sample(x,replace=TRUE) %>% mean ) %>% sd
```

- sd_boot   is: `r sd_boot`

```{r}
# CROSS-VALIDATION

# function to create a random (without replacement) partition of given size (partSize) from sample x
makePart<-function(partSize,x){
  xpart<-sample(x,partSize,replace=FALSE)
  return(xpart)
}

# function to remove a partition (xpart) from sample x and return the reduced sample (xnew)
removePart<-function(xpart,x){
  xnew<-x[!x %in% xpart]
  return(xnew)
}

# main function: 
# - compute partSize from input sample x size and number of folds (k) 
# - continue to create partitions and compute their mean while enough entries to do so
# - return the standard deviation of these k means
performCV<-function(x,k){
  partSize = floor(length(x)/k)
  xnew<-x
  means<-c()
  while(length(xnew) >= partSize){
    xpart<-makePart(partSize,xnew)
    means<-c(means,mean(xpart))
    xnew<-removePart(xpart,xnew)
  }
  return(sd(means))
}

sd_cv<-performCV(x,k)
```

- sd_cv   is: `r sd_cv`

- The bootstrap code returns a standard deviation of `2.21`, while the 10-fold cross validation code returns a standard deviation of `4.67`.  Varying the random number seed a repeating several times, the bootstrap results ranged from `0.9-2.2`, and the 10-fold CV code from `3.3-4.7`.  I also varied the size of x from 20 to 50, 100, to 1000.  The standard deviation decreases for both BS and CV, but CV remained larger in all cases.  The book mentioned that CV generally has a higher variance than other methods, but potentially a lower bias.  I also examined the mean of means, and saw no evidence of a bias for either methods.

# Exercise 2: Binomial Metrics

Here's a really simple Model of Versicolor iris based on the **iris** data :

```{r}
set.seed(1)
data(iris)

qplot( data=iris, x=Petal.Length, y=Sepal.Length, color=Species )

# Create Dependent Variable
iris$Versicolor <- 
  ifelse( iris$Species == 'versicolor', "versicolor", "other" ) %>% as.factor
iris$Species = NULL 

# Create Training and Test Samples
wh <- sample.int( nrow(iris), size=nrow(iris)/2 )
train <- iris[ wh,]
test <- iris[ -wh, ]

# Fit logistic model
fit.glm <- glm( Versicolor ~ . - Sepal.Length, data=train, family=binomial )
```

Use the models to and write functions to calculate:

* Prevalence 
* Accuracy
* Error Rate / Misclassification Rate
* True Positive Rate  
* False Positive Rate
* True Negative Rate  
* False Negative Rate 
* Sensitivity 
* Specificity 
* Recall 
* Precision

The functions should take two logical vectors of the same length, `y` and `yhat`

```{r}
threshold = 0.5
y = test$Versicolor == 'versicolor'
yhat = predict(fit.glm, test, type="response") > threshold

# prevalence [ = 0.293]
prevalence<-function(y,yhat){
  sum(y)/length(y)
}
# accuracy [ = 0.733 ]
accuracy<-function(y,yhat){
  (sum(y&yhat)+sum(!y&!yhat))/length(y)
}
# error_rate (also misclassificate rate) [ = 0.267 ]
error_rate<-function(y,yhat){
  (sum(y&!yhat)+sum(!y&yhat))/length(y)
}
# tpr (true positive rate, also sensitivity or recall) [ = 0.545 ] 
tpr<-function(y,yhat){
  sum(y&yhat)/sum(y)
}
# fpr (false positive rate) [ = 0.455 ]
fpr<-function(y,yhat){
  sum(y&!yhat)/sum(y)
}
# tnr (true negative rate, also specificity) [ = 0.811 ]
tnr<-function(y,yhat){
  sum(!y&!yhat)/sum(!y)
}
# fnr (false negative or miss rate) [ = 0.455 ]
fnr<-function(y,yhat){
  sum(y&!yhat)/sum(yhat)
}
# precision (also positive predicted value) [ = 0.545]
precision<-function(y,yhat){
  sum(y&yhat)/sum(yhat)
}
```

- What is wrong with the modeling approach used?

- This model to identify Versicolor, through grouping Setosa and Virginica as Other, and then using logistic regression, only correctly classifies true Versicolors about `50%` of the time (senistivity).  Non-versicolors are classified as other about `80%` of the time (specificity).  The threshold used was `0.5` and we could vary this to increase the sensitivity, but we will pay a price in specificity.  In general, we are not able to separate Versicolor from Virginica and Setosa, because Versicolor is generally between these other two species in the discriminating variables.  The other category essenitially averages Setosa and Viriginca, which looks a lot like Versicolor. 
